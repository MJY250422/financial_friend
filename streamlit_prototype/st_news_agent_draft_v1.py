import streamlit as st
import pandas as pd
import feedparser
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import sqlite3
from dataclasses import dataclass, asdict
import re
from collections import defaultdict

# ============================================================================
# ë°ì´í„° ëª¨ë¸
# ============================================================================

@dataclass
class Article:
    article_id: str
    source: str
    url: str
    title: str
    published_at: str
    content: str
    summary: str
    top_facts: str
    simhash: str
    tokens_used: int
    created_at: str

@dataclass
class FetchLog:
    log_id: str
    timestamp: str
    log_type: str
    status: str
    details: str
    source: Optional[str] = None

@dataclass
class ModelRun:
    run_id: str
    timestamp: str
    model_name: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    article_id: Optional[str] = None
    query: Optional[str] = None

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
# ============================================================================

def init_database():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    conn = sqlite3.connect('news_agent.db', check_same_thread=False)
    cursor = conn.cursor()
    
    # articles í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS articles (
            article_id TEXT PRIMARY KEY,
            source TEXT NOT NULL,
            url TEXT NOT NULL,
            title TEXT NOT NULL,
            published_at TEXT NOT NULL,
            content TEXT,
            summary TEXT,
            top_facts TEXT,
            simhash TEXT,
            tokens_used INTEGER,
            created_at TEXT NOT NULL
        )
    ''')
    
    # fetch_logs í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS fetch_logs (
            log_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            log_type TEXT NOT NULL,
            status TEXT NOT NULL,
            details TEXT,
            source TEXT
        )
    ''')
    
    # model_runs í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS model_runs (
            run_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            model_name TEXT NOT NULL,
            prompt_tokens INTEGER,
            completion_tokens INTEGER,
            total_tokens INTEGER,
            article_id TEXT,
            query TEXT,
            FOREIGN KEY (article_id) REFERENCES articles(article_id)
        )
    ''')
    
    # rag_eval í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS rag_eval (
            eval_id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            query TEXT,
            retrieved_count INTEGER,
            relevance_score REAL,
            response_quality TEXT
        )
    ''')
    
    conn.commit()
    return conn

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def generate_simhash(text: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ simhash ìƒì„± (ì¤‘ë³µ ê°ì§€ìš©)"""
    hash_obj = hashlib.md5(text.encode('utf-8'))
    return hash_obj.hexdigest()[:16]

def estimate_tokens(text: str) -> int:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì • (1,000ì â‰ˆ 500 tokens)"""
    return int(len(text) * 0.5)

def normalize_url(url: str) -> str:
    """URL ì •ê·œí™”"""
    url = url.strip()
    url = re.sub(r'[?#].*$', '', url)
    return url

def extract_text_from_html(html_content: str) -> str:
    """HTMLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜"""
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” readability-lxml ë˜ëŠ” newspaper3k ì‚¬ìš©
    text = re.sub(r'<[^>]+>', ' ', html_content)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# ============================================================================
# RSS ë° ê¸°ì‚¬ ìˆ˜ì§‘
# ============================================================================

RSS_SOURCES = [
    {
        'name': 'ë§¤ì¼ê²½ì œ',
        'url': 'https://www.mk.co.kr/rss/30300001/',
        'category': 'economy'
    },
    {
        'name': 'í•œêµ­ê²½ì œ',
        'url': 'https://www.hankyung.com/feed/economy',
        'category': 'economy'
    },
    {
        'name': 'KBSë‰´ìŠ¤',
        'url': 'https://fs.kbs.co.kr/rss/news.xml',
        'category': 'general'
    }
]

def fetch_rss_articles(source: Dict, max_articles: int = 10) -> List[Dict]:
    """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
    try:
        feed = feedparser.parse(source['url'])
        articles = []
        
        for entry in feed.entries[:max_articles]:
            article = {
                'source': source['name'],
                'url': normalize_url(entry.get('link', '')),
                'title': entry.get('title', 'ì œëª© ì—†ìŒ'),
                'published_at': entry.get('published', datetime.now().isoformat()),
                'content': entry.get('summary', entry.get('description', ''))
            }
            articles.append(article)
        
        return articles
    except Exception as e:
        st.error(f"RSS ìˆ˜ì§‘ ì˜¤ë¥˜ ({source['name']}): {str(e)}")
        return []

def generate_llm_summary(content: str, source: str) -> tuple:
    """LLMì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„± ì‹œë®¬ë ˆì´ì…˜"""
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” OpenAI API ë˜ëŠ” ë‹¤ë¥¸ LLM í˜¸ì¶œ
    
    # í† í° ê³„ì‚°
    prompt_tokens = estimate_tokens(f"ë‹¤ìŒ ê¸°ì‚¬ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ê³  í•µì‹¬ ìˆ˜ì¹˜ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:\n{content}")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM ì‘ë‹µ)
    sentences = content.split('.')[:3]
    summary = '. '.join(sentences).strip() + '.'
    
    # í•µì‹¬ ì‚¬ì‹¤ ì¶”ì¶œ
    top_facts = f"""â€¢ ì£¼ìš” ë³€ë™: {hash(content) % 100}% ë³€í™”
â€¢ ë¶„ì„ ê¸°ê°„: ìµœê·¼ {hash(content) % 12 + 1}ê°œì›”
â€¢ ì¶œì²˜: {source}"""
    
    completion_tokens = estimate_tokens(summary + top_facts)
    
    return summary, top_facts, prompt_tokens, completion_tokens

def deduplicate_articles(articles: List[Dict], existing_hashes: set) -> List[Dict]:
    """simhash ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
    unique_articles = []
    seen_hashes = existing_hashes.copy()
    
    for article in articles:
        article_hash = generate_simhash(article['title'] + article['url'])
        if article_hash not in seen_hashes:
            article['simhash'] = article_hash
            unique_articles.append(article)
            seen_hashes.add(article_hash)
    
    return unique_articles

# ============================================================================
# ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤
# ============================================================================

def run_batch_process(conn: sqlite3.Connection, settings: Dict) -> Dict:
    """ë°°ì¹˜í˜• ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½"""
    cursor = conn.cursor()
    start_time = time.time()
    
    # ë¡œê·¸ ê¸°ë¡
    batch_log_id = f"batch_{int(time.time() * 1000)}"
    log_entry = FetchLog(
        log_id=batch_log_id,
        timestamp=datetime.now().isoformat(),
        log_type='batch',
        status='started',
        details=f"ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ - {len(RSS_SOURCES)}ê°œ ì†ŒìŠ¤"
    )
    cursor.execute(
        'INSERT INTO fetch_logs VALUES (?, ?, ?, ?, ?, ?)',
        tuple(asdict(log_entry).values())
    )
    conn.commit()
    
    # ê¸°ì¡´ simhash ê°€ì ¸ì˜¤ê¸°
    cursor.execute('SELECT simhash FROM articles')
    existing_hashes = {row[0] for row in cursor.fetchall()}
    
    all_articles = []
    total_tokens = 0
    fetch_results = []
    
    # ê° RSS ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
    for source in RSS_SOURCES:
        try:
            raw_articles = fetch_rss_articles(source, settings['articles_per_source'])
            unique_articles = deduplicate_articles(raw_articles, existing_hashes)
            
            # ê° ê¸°ì‚¬ ì²˜ë¦¬
            for article in unique_articles:
                # ë³¸ë¬¸ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” readability ì‚¬ìš©)
                content = extract_text_from_html(article['content'])
                
                # LLM ìš”ì•½
                summary, top_facts, prompt_tok, comp_tok = generate_llm_summary(
                    content, article['source']
                )
                
                # Article ê°ì²´ ìƒì„±
                article_id = f"{article['source']}_{generate_simhash(article['url'])}"
                article_obj = Article(
                    article_id=article_id,
                    source=article['source'],
                    url=article['url'],
                    title=article['title'],
                    published_at=article['published_at'],
                    content=content[:5000],  # ìµœëŒ€ 5000ì
                    summary=summary,
                    top_facts=top_facts,
                    simhash=article['simhash'],
                    tokens_used=prompt_tok + comp_tok,
                    created_at=datetime.now().isoformat()
                )
                
                # DB ì‚½ì… (UPSERT)
                cursor.execute('''
                    INSERT OR REPLACE INTO articles 
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', tuple(asdict(article_obj).values()))
                
                # Model run ê¸°ë¡
                run_id = f"run_{int(time.time() * 1000)}_{article_id}"
                model_run = ModelRun(
                    run_id=run_id,
                    timestamp=datetime.now().isoformat(),
                    model_name='gpt-4o-mini',  # ì‹¤ì œ ëª¨ë¸ëª…
                    prompt_tokens=prompt_tok,
                    completion_tokens=comp_tok,
                    total_tokens=prompt_tok + comp_tok,
                    article_id=article_id
                )
                cursor.execute(
                    'INSERT INTO model_runs VALUES (?, ?, ?, ?, ?, ?, ?, ?)',
                    tuple(asdict(model_run).values())
                )
                
                all_articles.append(article_obj)
                total_tokens += article_obj.tokens_used
                existing_hashes.add(article['simhash'])
            
            # ì†ŒìŠ¤ë³„ ë¡œê·¸
            fetch_log = FetchLog(
                log_id=f"fetch_{source['name']}_{int(time.time() * 1000)}",
                timestamp=datetime.now().isoformat(),
                log_type='fetch',
                status='success',
                details=f"{len(unique_articles)}ê±´ ìˆ˜ì§‘ (ì¤‘ë³µ {len(raw_articles) - len(unique_articles)}ê±´ ì œê±°)",
                source=source['name']
            )
            cursor.execute(
                'INSERT INTO fetch_logs VALUES (?, ?, ?, ?, ?, ?)',
                tuple(asdict(fetch_log).values())
            )
            
            fetch_results.append({
                'source': source['name'],
                'collected': len(unique_articles),
                'duplicates': len(raw_articles) - len(unique_articles)
            })
            
        except Exception as e:
            error_log = FetchLog(
                log_id=f"error_{source['name']}_{int(time.time() * 1000)}",
                timestamp=datetime.now().isoformat(),
                log_type='fetch',
                status='error',
                details=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                source=source['name']
            )
            cursor.execute(
                'INSERT INTO fetch_logs VALUES (?, ?, ?, ?, ?, ?)',
                tuple(asdict(error_log).values())
            )
    
    conn.commit()
    
    # ì™„ë£Œ ë¡œê·¸
    elapsed_time = time.time() - start_time
    complete_log = FetchLog(
        log_id=f"batch_complete_{int(time.time() * 1000)}",
        timestamp=datetime.now().isoformat(),
        log_type='batch',
        status='completed',
        details=f"{len(all_articles)}ê±´ ì²˜ë¦¬, {total_tokens:,} í† í° ì‚¬ìš©, {elapsed_time:.1f}ì´ˆ ì†Œìš”"
    )
    cursor.execute(
        'INSERT INTO fetch_logs VALUES (?, ?, ?, ?, ?, ?)',
        tuple(asdict(complete_log).values())
    )
    conn.commit()
    
    return {
        'articles_count': len(all_articles),
        'total_tokens': total_tokens,
        'elapsed_time': elapsed_time,
        'fetch_results': fetch_results
    }

# ============================================================================
# ì˜¨ë””ë§¨ë“œ ì¿¼ë¦¬
# ============================================================================

def process_ondemand_query(conn: sqlite3.Connection, query: str, settings: Dict) -> Dict:
    """ì˜¨ë””ë§¨ë“œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    cursor = conn.cursor()
    start_time = time.time()
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = [word.lower() for word in query.split() if len(word) > 1]
    
    # DBì—ì„œ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
    time_window = datetime.now() - timedelta(hours=settings['time_window'])
    
    cursor.execute('''
        SELECT * FROM articles 
        WHERE published_at >= ? 
        ORDER BY published_at DESC 
        LIMIT 20
    ''', (time_window.isoformat(),))
    
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    
    # ê´€ë ¨ë„ ê³„ì‚°
    relevant_articles = []
    for row in rows:
        article = dict(zip(columns, row))
        relevance_score = sum(
            1 for kw in keywords 
            if kw in article['title'].lower() or kw in article['summary'].lower()
        )
        if relevance_score > 0 or len(relevant_articles) < 5:
            article['relevance_score'] = relevance_score
            relevant_articles.append(article)
    
    # ê´€ë ¨ë„ ìˆœ ì •ë ¬
    relevant_articles.sort(key=lambda x: x['relevance_score'], reverse=True)
    top_articles = relevant_articles[:5]
    
    # ì—†ìœ¼ë©´ ì‹¤ì‹œê°„ ìˆ˜ì§‘
    if len(top_articles) == 0:
        st.warning("DBì— ê´€ë ¨ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        for source in RSS_SOURCES[:2]:
            raw_articles = fetch_rss_articles(source, 5)
            for article in raw_articles[:3]:
                content = extract_text_from_html(article['content'])
                summary, top_facts, _, _ = generate_llm_summary(content, article['source'])
                top_articles.append({
                    'source': article['source'],
                    'url': article['url'],
                    'title': article['title'],
                    'summary': summary,
                    'top_facts': top_facts,
                    'relevance_score': 0
                })
    
    # LLM ì‘ë‹µ ìƒì„±
    context = "\n\n".join([
        f"[{i+1}] {a['title']}\n{a['summary']}\nì¶œì²˜: {a['source']}"
        for i, a in enumerate(top_articles)
    ])
    
    prompt = f"ì§ˆë¬¸: {query}\n\nê´€ë ¨ ê¸°ì‚¬:\n{context}"
    prompt_tokens = estimate_tokens(prompt)
    
    # ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” LLM í˜¸ì¶œ)
    response = f"""ì§ˆë¬¸í•˜ì‹  '{query}'ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.\n\n"""
    
    for i, article in enumerate(top_articles, 1):
        response += f"**{i}. {article['title']}**\n"
        response += f"{article['summary']}\n"
        response += f"[ì¶œì²˜: {article['source']}]({article['url']})\n\n"
    
    response += f"---\nğŸ’¡ **ì¢…í•© ë¶„ì„**: {len(top_articles)}ê°œ ê¸°ì‚¬ë¥¼ ë¶„ì„í•œ ê²°ê³¼, "
    response += f"ìµœê·¼ {settings['time_window']}ì‹œê°„ ë™ì•ˆì˜ ì£¼ìš” ë™í–¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.\n\n"
    
    completion_tokens = estimate_tokens(response)
    total_tokens = prompt_tokens + completion_tokens
    
    # Model run ê¸°ë¡
    run_id = f"query_{int(time.time() * 1000)}"
    model_run = ModelRun(
        run_id=run_id,
        timestamp=datetime.now().isoformat(),
        model_name='gpt-4o-mini',
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        total_tokens=total_tokens,
        query=query
    )
    cursor.execute(
        'INSERT INTO model_runs VALUES (?, ?, ?, ?, ?, ?, ?, ?)',
        tuple(asdict(model_run).values())
    )
    
    # RAG í‰ê°€ ê¸°ë¡
    eval_id = f"eval_{int(time.time() * 1000)}"
    cursor.execute('''
        INSERT INTO rag_eval VALUES (?, ?, ?, ?, ?, ?)
    ''', (
        eval_id,
        datetime.now().isoformat(),
        query,
        len(top_articles),
        sum(a['relevance_score'] for a in top_articles) / max(len(top_articles), 1),
        'good'
    ))
    
    # ì¿¼ë¦¬ ë¡œê·¸
    query_log = FetchLog(
        log_id=f"query_{int(time.time() * 1000)}",
        timestamp=datetime.now().isoformat(),
        log_type='query',
        status='success',
        details=f"'{query}' - {len(top_articles)}ê±´ ì°¸ì¡°, {total_tokens:,} í† í° ì‚¬ìš©"
    )
    cursor.execute(
        'INSERT INTO fetch_logs VALUES (?, ?, ?, ?, ?, ?)',
        tuple(asdict(query_log).values())
    )
    
    conn.commit()
    
    return {
        'response': response,
        'articles': top_articles,
        'tokens': total_tokens,
        'elapsed_time': time.time() - start_time
    }

# ============================================================================
# Streamlit UI
# ============================================================================

def main():
    st.set_page_config(
        page_title="ë‰´ìŠ¤ ì—ì´ì „íŠ¸",
        page_icon="ğŸ“°",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # DB ì´ˆê¸°í™”
    if 'conn' not in st.session_state:
        st.session_state.conn = init_database()
    
    conn = st.session_state.conn
    cursor = conn.cursor()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.title("ğŸ“° ë‰´ìŠ¤ ì—ì´ì „íŠ¸")
        st.markdown("**ë°°ì¹˜í˜• + ì˜¨ë””ë§¨ë“œí˜•**")
        st.divider()
        
        st.subheader("âš™ï¸ ì„¤ì •")
        
        batch_interval = st.number_input(
            "ë°°ì¹˜ ì£¼ê¸° (ë¶„)",
            min_value=5,
            max_value=1440,
            value=30,
            help="ìë™ ë°°ì¹˜ ì‹¤í–‰ ê°„ê²©"
        )
        
        articles_per_source = st.number_input(
            "ì†ŒìŠ¤ë‹¹ ê¸°ì‚¬ ìˆ˜",
            min_value=1,
            max_value=50,
            value=5,
            help="ê° RSS ì†ŒìŠ¤ì—ì„œ ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜"
        )
        
        time_window = st.number_input(
            "ì‹œê°„ ìœˆë„ìš° (ì‹œê°„)",
            min_value=1,
            max_value=48,
            value=3,
            help="í‘œì‹œí•  ê¸°ì‚¬ì˜ ì‹œê°„ ë²”ìœ„"
        )
        
        settings = {
            'batch_interval': batch_interval,
            'articles_per_source': articles_per_source,
            'time_window': time_window
        }
        
        st.divider()
        
        if st.button("ğŸ”„ ë°°ì¹˜ ì‹¤í–‰", use_container_width=True, type="primary"):
            with st.spinner("ë°°ì¹˜ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘..."):
                result = run_batch_process(conn, settings)
                st.success(f"âœ… {result['articles_count']}ê±´ ì²˜ë¦¬ ì™„ë£Œ!")
                st.info(f"í† í°: {result['total_tokens']:,} | ì‹œê°„: {result['elapsed_time']:.1f}ì´ˆ")
        
        st.divider()
        st.caption("ğŸ’¡ ìë™ ë°°ì¹˜ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤")
    
    # ë©”ì¸ íƒ­
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š ëŒ€ì‹œë³´ë“œ", "ğŸ’¬ ì±—ë´‡ ìš”ì•½", "ğŸ“‹ ì‹¤í–‰ ë¡œê·¸", "ğŸ“ˆ í†µê³„"])
    
    # íƒ­ 1: ëŒ€ì‹œë³´ë“œ
    with tab1:
        st.header("ìµœê·¼ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ")
        
        # í†µê³„ ì¹´ë“œ
        col1, col2, col3, col4 = st.columns(4)
        
        cursor.execute('SELECT COUNT(*) FROM articles')
        total_articles = cursor.fetchone()[0]
        
        cursor.execute('SELECT SUM(tokens_used) FROM articles')
        total_tokens = cursor.fetchone()[0] or 0
        
        cursor.execute('SELECT AVG(tokens_used) FROM articles')
        avg_tokens = cursor.fetchone()[0] or 0
        
        cursor.execute('SELECT MAX(created_at) FROM articles')
        last_batch = cursor.fetchone()[0]
        
        with col1:
            st.metric("ì´ ê¸°ì‚¬ ìˆ˜", f"{total_articles:,}")
        with col2:
            st.metric("ì´ í† í°", f"{total_tokens:,}")
        with col3:
            st.metric("í‰ê·  í† í°/ê¸°ì‚¬", f"{int(avg_tokens):,}")
        with col4:
            if last_batch:
                st.metric("ë§ˆì§€ë§‰ ë°°ì¹˜", datetime.fromisoformat(last_batch).strftime("%H:%M"))
            else:
                st.metric("ë§ˆì§€ë§‰ ë°°ì¹˜", "N/A")
        
        st.divider()
        
        # ê¸°ì‚¬ ëª©ë¡
        time_cutoff = datetime.now() - timedelta(hours=time_window)
        cursor.execute('''
            SELECT * FROM articles 
            WHERE published_at >= ? 
            ORDER BY published_at DESC
        ''', (time_cutoff.isoformat(),))
        
        articles = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        if len(articles) == 0:
            st.warning("í‘œì‹œí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë°°ì¹˜ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        else:
            st.subheader(f"ìµœê·¼ {time_window}ì‹œê°„ ë‰´ìŠ¤ ({len(articles)}ê±´)")
            
            for row in articles:
                article = dict(zip(columns, row))
                
                with st.expander(f"**{article['title']}** - {article['source']}", expanded=False):
                    col_a, col_b = st.columns([3, 1])
                    
                    with col_a:
                        st.markdown(f"**ğŸ“… ë°œí–‰ì‹œê°:** {datetime.fromisoformat(article['published_at']).strftime('%Y-%m-%d %H:%M')}")
                        st.markdown(f"**ğŸ“ ìš”ì•½:**")
                        st.write(article['summary'])
                        
                        st.markdown("**ğŸ“Š í•µì‹¬ ì‚¬ì‹¤:**")
                        st.info(article['top_facts'])
                        
                        st.markdown(f"[ì›ë¬¸ ë³´ê¸° â†’]({article['url']})")
                    
                    with col_b:
                        st.metric("í† í°", f"{article['tokens_used']:,}")
                        st.caption(f"ID: {article['article_id'][:20]}...")
    
    # íƒ­ 2: ì±—ë´‡
    with tab2:
        st.header("ğŸ’¬ ì˜¨ë””ë§¨ë“œ ë‰´ìŠ¤ ìš”ì•½")
        st.caption("ê¶ê¸ˆí•œ ë‰´ìŠ¤ ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìš”ì•½í•´ë“œë¦½ë‹ˆë‹¤.")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        if 'chat_history' not in st.session_state:
            st.session_state.chat_history = []
        
        # ì±„íŒ… ì…ë ¥
        user_query = st.chat_input("ì˜ˆ: ì˜¤ëŠ˜ ê¸ˆìœµë‰´ìŠ¤ ìš”ì•½, ë°˜ë„ì²´ ê´€ë ¨ ìµœì‹  ì†Œì‹")
        
        if user_query:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.chat_history.append({
                'role': 'user',
                'content': user_query,
                'timestamp': datetime.now()
            })
            
            # ì‘ë‹µ ìƒì„±
            with st.spinner("ë¶„ì„ ì¤‘..."):
                result = process_ondemand_query(conn, user_query, settings)
            
            # ì‘ë‹µ ì¶”ê°€
            st.session_state.chat_history.append({
                'role': 'assistant',
                'content': result['response'],
                'timestamp': datetime.now(),
                'tokens': result['tokens'],
                'elapsed': result['elapsed_time']
            })
        
        # ì±„íŒ… í‘œì‹œ
        for msg in st.session_state.chat_history:
            with st.chat_message(msg['role']):
                st.markdown(msg['content'])
                if msg['role'] == 'assistant':
                    st.caption(f"â±ï¸ {msg['elapsed']:.1f}ì´ˆ | ğŸ¯ {msg['tokens']:,} tokens | ğŸ• {msg['timestamp'].strftime('%H:%M:%S')}")
    
    # íƒ­ 3: ë¡œê·¸
    with tab3:
        st.header("ğŸ“‹ ì‹¤í–‰ ë¡œê·¸")
        
        log_type_filter = st.multiselect(
            "ë¡œê·¸ íƒ€ì… í•„í„°",
            ['batch', 'fetch', 'query', 'error'],
            default=['batch', 'fetch', 'query']
        )
        
        placeholders = ','.join(['?' for _ in log_type_filter])
        cursor.execute(f'''
            SELECT * FROM fetch_logs 
            WHERE log_type IN ({placeholders})
            ORDER BY timestamp DESC 
            LIMIT 100
        ''', tuple(log_type_filter))
        
        logs = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        if len(logs) == 0:
            st.info("ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for row in logs:
                log = dict(zip(columns, row))
                
                status_emoji = {
                    'started': 'â–¶ï¸',
                    'success': 'âœ…',
                    'completed': 'âœ…',
                    'error': 'âŒ',
                    'running': 'ğŸ”„'
                }.get(log['status'], 'â€¢')
                
                type_color = {
                    'batch': 'blue',
                    'fetch': 'green',
                    'query': 'orange',
                    'error': 'red'
                }.get(log['log_type'], 'gray')
                
                with st.container():
                    col1, col2, col3 = st.columns([1, 2, 5])
                    with col1:
                        st.markdown(f":{type_color}[**{log['log_type'].upper()}**]")
                    with col2:
                        st.caption(datetime.fromisoformat(log['timestamp']).strftime('%Y-%m-%d %H:%M:%S'))
                    with col3:
                        st.markdown(f"{status_emoji} {log['details']}")
                    st.divider()
    
    # íƒ­ 4: í†µê³„
    with tab4:
        st.header("ğŸ“ˆ í†µê³„ ë° ë¶„ì„")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ìˆ˜")
            cursor.execute('''
                SELECT source, COUNT(*) as count 
                FROM articles 
                GROUP BY source 
                ORDER BY count DESC
            ''')
            source_stats = cursor.fetchall()
            if source_stats:
                df_sources = pd.DataFrame(source_stats, columns=['ì†ŒìŠ¤', 'ê¸°ì‚¬ ìˆ˜'])
                st.bar_chart(df_sources.set_index('ì†ŒìŠ¤'))
            else:
                st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        with col2:
            st.subheader("í† í° ì‚¬ìš© ì¶”ì´")
            cursor.execute('''
                SELECT DATE(timestamp) as date, SUM(total_tokens) as tokens
                FROM model_runs
                GROUP BY DATE(timestamp)
                ORDER BY date DESC
                LIMIT 7
            ''')
            token_stats = cursor.fetchall()
            if token_stats:
                df_tokens = pd.DataFrame(token_stats, columns=['ë‚ ì§œ', 'í† í°'])
                st.line_chart(df_tokens.set_index('ë‚ ì§œ'))
            else:
                st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        st.divider()
        
        st.subheader("ğŸ“Š ì „ì²´ ëª¨ë¸ ì‹¤í–‰ í†µê³„")
        cursor.execute('''
            SELECT 
                COUNT(*) as total_runs,
                SUM(total_tokens) as total_tokens,
                AVG(total_tokens) as avg_tokens,
                SUM(prompt_tokens) as prompt_tokens,
                SUM(completion_tokens) as completion_tokens
            FROM model_runs
        ''')
        model_stats = cursor.fetchone()
        
        if model_stats and model_stats[0] > 0:
            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("ì´ ì‹¤í–‰ íšŸìˆ˜", f"{model_stats[0]:,}")
            with col2:
                st.metric("ì´ í† í°", f"{model_stats[1]:,}")
            with col3:
                st.metric("í‰ê·  í† í°", f"{int(model_stats[2]):,}")
            with col4:
                st.metric("í”„ë¡¬í”„íŠ¸ í† í°", f"{model_stats[3]:,}")
            with col5:
                st.metric("ì™„ì„± í† í°", f"{model_stats[4]:,}")
        else:
            st.info("ëª¨ë¸ ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        st.divider()
        
        # RAG í‰ê°€ í†µê³„
        st.subheader("ğŸ¯ RAG í‰ê°€ í†µê³„")
        cursor.execute('''
            SELECT 
                AVG(retrieved_count) as avg_retrieved,
                AVG(relevance_score) as avg_relevance,
                COUNT(*) as total_queries
            FROM rag_eval
        ''')
        rag_stats = cursor.fetchone()
        
        if rag_stats and rag_stats[2] > 0:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì¿¼ë¦¬ ìˆ˜", f"{rag_stats[2]:,}")
            with col2:
                st.metric("í‰ê·  ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜", f"{rag_stats[0]:.1f}")
            with col3:
                st.metric("í‰ê·  ê´€ë ¨ë„ ì ìˆ˜", f"{rag_stats[1]:.2f}")
        else:
            st.info("RAG í‰ê°€ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        st.divider()
        
        # ìµœê·¼ í™œë™
        st.subheader("ğŸ•’ ìµœê·¼ í™œë™")
        cursor.execute('''
            SELECT timestamp, model_name, total_tokens, query, article_id
            FROM model_runs
            ORDER BY timestamp DESC
            LIMIT 10
        ''')
        recent_runs = cursor.fetchall()
        
        if recent_runs:
            df_recent = pd.DataFrame(
                recent_runs,
                columns=['ì‹œê°', 'ëª¨ë¸', 'í† í°', 'ì¿¼ë¦¬', 'ê¸°ì‚¬ID']
            )
            df_recent['ì‹œê°'] = pd.to_datetime(df_recent['ì‹œê°']).dt.strftime('%Y-%m-%d %H:%M:%S')
            df_recent['ì¿¼ë¦¬'] = df_recent['ì¿¼ë¦¬'].fillna('-')
            df_recent['ê¸°ì‚¬ID'] = df_recent['ê¸°ì‚¬ID'].fillna('-').str[:20]
            st.dataframe(df_recent, use_container_width=True)
        else:
            st.info("ìµœê·¼ í™œë™ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()