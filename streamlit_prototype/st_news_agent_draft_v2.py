"""
ë‰´ìŠ¤ ì—ì´ì „íŠ¸ í”„ë¡œí† íƒ€ì… (Streamlit)
- ë°°ì¹˜í˜•: RSS ìˆ˜ì§‘ â†’ ì¤‘ë³µ ì œê±° â†’ ìš”ì•½ â†’ DB ì ì¬
- ì˜¨ë””ë§¨ë“œí˜•: ì‹¤ì‹œê°„ ì§ˆì˜ â†’ ë¬¸ì„œ ê²€ìƒ‰ â†’ LLM ì‘ë‹µ
"""

import streamlit as st
import pandas as pd
import hashlib
from datetime import datetime, timedelta
import time
import re
from typing import List, Dict, Tuple

# ============================================================================
# CONFIGURATION & CONSTANTS
# ============================================================================

DEFAULT_RSS_FEEDS = [
    "https://www.mk.co.kr/rss/30100041/",  # ë§¤ì¼ê²½ì œ ê²½ì œ
    "https://www.hankyung.com/feed/economy",  # í•œêµ­ê²½ì œ
    "https://www.sedaily.com/RSS/S1N1.xml"  # ì„œìš¸ê²½ì œ
]

KOREAN_CHAR_TO_TOKEN_RATIO = 2.0  # í•œê¸€ 1,000ì â‰ˆ 500 tokens
AVG_ARTICLE_LENGTH = 3000  # í‰ê·  ê¸°ì‚¬ ê¸¸ì´ (ì)
BATCH_INTERVAL_MINUTES = 30

# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def calculate_simhash(text: str, bit_length: int = 64) -> int:
    """
    SimHash ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ í•´ì‹œ ìƒì„±
    ì¤‘ë³µ ê¸°ì‚¬ íƒì§€ìš©
    """
    tokens = re.findall(r'\w+', text.lower())
    hash_vec = [0] * bit_length
    
    for token in tokens:
        h = int(hashlib.md5(token.encode()).hexdigest(), 16)
        for i in range(bit_length):
            if h & (1 << i):
                hash_vec[i] += 1
            else:
                hash_vec[i] -= 1
    
    fingerprint = 0
    for i in range(bit_length):
        if hash_vec[i] > 0:
            fingerprint |= (1 << i)
    
    return fingerprint


def hamming_distance(hash1: int, hash2: int) -> int:
    """ë‘ SimHash ê°„ í•´ë° ê±°ë¦¬ ê³„ì‚°"""
    xor = hash1 ^ hash2
    distance = 0
    while xor:
        distance += 1
        xor &= xor - 1
    return distance


def estimate_tokens(text: str) -> int:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì •"""
    char_count = len(text)
    return int(char_count / KOREAN_CHAR_TO_TOKEN_RATIO)


def normalize_url(url: str) -> str:
    """URL ì •ê·œí™” (íŒŒë¼ë¯¸í„° ì œê±°, ì†Œë¬¸ìí™”)"""
    url = url.lower().strip()
    url = re.sub(r'\?.*$', '', url)  # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    url = re.sub(r'#.*$', '', url)  # ì•µì»¤ ì œê±°
    return url


# ============================================================================
# MOCK DATA GENERATORS (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” RSS/DB/LLM ì—°ë™)
# ============================================================================

def fetch_rss_articles(feed_urls: List[str], limit_per_feed: int = 10) -> List[Dict]:
    """
    RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
    ì‹¤ì œ êµ¬í˜„: feedparser.parse() ì‚¬ìš©
    """
    mock_articles = []
    sources = ["ë§¤ì¼ê²½ì œ", "í•œêµ­ê²½ì œ", "ì„œìš¸ê²½ì œ"]
    
    for i, feed_url in enumerate(feed_urls[:3]):
        for j in range(limit_per_feed):
            article = {
                'source': sources[i],
                'url': f"https://example.com/article/{i}_{j}",
                'title': f"[{sources[i]}] {['ê¸ˆë¦¬ ì¸ìƒ', 'ë°˜ë„ì²´ ìˆ˜ì¶œ ê¸‰ì¦', 'ë¶€ë™ì‚° ì‹œì¥ ë™í–¥'][i]} ê´€ë ¨ ê¸°ì‚¬ #{j+1}",
                'published_at': datetime.now() - timedelta(hours=i*2 + j),
                'raw_content': f"ì´ ê¸°ì‚¬ëŠ” {sources[i]}ì—ì„œ ë°œí–‰í•œ {'ê²½ì œ' if i==0 else 'ê¸ˆìœµ'} ê´€ë ¨ ë‰´ìŠ¤ì…ë‹ˆë‹¤. " * 50
            }
            mock_articles.append(article)
    
    return mock_articles


def extract_article_body(url: str) -> str:
    """
    URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
    ì‹¤ì œ êµ¬í˜„: readability-lxml ë˜ëŠ” newspaper3k ì‚¬ìš©
    """
    # Mock implementation
    return f"ë³¸ë¬¸ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ëŠ” ê²½ì œ ë™í–¥ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. " * 30


def summarize_with_llm(content: str, source: str) -> Tuple[str, str]:
    """
    LLMì„ í†µí•œ ê¸°ì‚¬ ìš”ì•½
    ì‹¤ì œ êµ¬í˜„: OpenAI API ë˜ëŠ” Anthropic Claude API
    
    Returns:
        (summary, top_facts) íŠœí”Œ
    """
    # Mock LLM response
    summary = f"ì´ ê¸°ì‚¬ëŠ” ìµœê·¼ ê²½ì œ ë™í–¥ì„ ë‹¤ë£¨ë©°, ì£¼ìš” ì§€í‘œì˜ ë³€í™”ì™€ ì „ë§ì„ ë¶„ì„í•©ë‹ˆë‹¤. " \
              f"ì „ë¬¸ê°€ë“¤ì€ í–¥í›„ 3ê°œì›”ê°„ ì™„ë§Œí•œ ì„±ì¥ì„¸ë¥¼ ì˜ˆìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. " \
              f"[ì¶œì²˜: {source}]"
    
    top_facts = "â€¢ GDP ì„±ì¥ë¥  2.3% ê¸°ë¡\nâ€¢ ìˆ˜ì¶œ ì „ë…„ ëŒ€ë¹„ 15% ì¦ê°€\nâ€¢ ì‹¤ì—…ë¥  3.1%ë¡œ ì†Œí­ í•˜ë½"
    
    return summary, top_facts


def upsert_articles_to_db(articles: List[Dict]) -> int:
    """
    ê¸°ì‚¬ ë°ì´í„°ë¥¼ DBì— UPSERT
    ì‹¤ì œ êµ¬í˜„: SQLite, PostgreSQL ë“± ì‚¬ìš©
    
    Returns:
        ì‚½ì…/ì—…ë°ì´íŠ¸ëœ ë ˆì½”ë“œ ìˆ˜
    """
    # Mock DB operation
    if 'db_articles' not in st.session_state:
        st.session_state.db_articles = []
    
    inserted = 0
    for article in articles:
        # ì¤‘ë³µ ì²´í¬ (URL ê¸°ì¤€)
        existing = next((a for a in st.session_state.db_articles 
                        if a['url'] == article['url']), None)
        if not existing:
            st.session_state.db_articles.append(article)
            inserted += 1
    
    return inserted


def log_batch_run(run_id: str, discovered: int, inserted: int, start_time: datetime):
    """ë°°ì¹˜ ì‹¤í–‰ ë¡œê·¸ ê¸°ë¡"""
    if 'fetch_logs' not in st.session_state:
        st.session_state.fetch_logs = []
    
    log_entry = {
        'run_id': run_id,
        'started_at': start_time,
        'finished_at': datetime.now(),
        'discovered_count': discovered,
        'inserted_count': inserted
    }
    st.session_state.fetch_logs.append(log_entry)


# ============================================================================
# CORE BUSINESS LOGIC
# ============================================================================

def batch_process_pipeline(feed_urls: List[str], limit_per_feed: int = 10) -> Dict:
    """
    ë°°ì¹˜í˜• íŒŒì´í”„ë¼ì¸ ì „ì²´ íë¦„
    1. RSS ìˆ˜ì§‘
    2. ì¤‘ë³µ ì œê±° (SimHash)
    3. ë³¸ë¬¸ ì¶”ì¶œ
    4. LLM ìš”ì•½
    5. DB ì ì¬
    """
    start_time = datetime.now()
    run_id = hashlib.md5(str(start_time).encode()).hexdigest()[:16]
    
    # Step 1: RSS ìˆ˜ì§‘
    raw_articles = fetch_rss_articles(feed_urls, limit_per_feed)
    
    # Step 2: ì •ê·œí™” ë° ì¤‘ë³µ ì œê±°
    normalized_articles = []
    seen_hashes = set()
    
    for article in raw_articles:
        article['url'] = normalize_url(article['url'])
        article['simhash'] = calculate_simhash(article['title'] + article['raw_content'])
        
        # SimHash ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ (í•´ë° ê±°ë¦¬ < 3ì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)
        is_duplicate = False
        for seen_hash in seen_hashes:
            if hamming_distance(article['simhash'], seen_hash) < 3:
                is_duplicate = True
                break
        
        if not is_duplicate:
            seen_hashes.add(article['simhash'])
            normalized_articles.append(article)
    
    # Step 3-4: ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½
    processed_articles = []
    total_tokens = 0
    
    for article in normalized_articles:
        full_content = extract_article_body(article['url'])
        summary, top_facts = summarize_with_llm(full_content, article['source'])
        
        article_record = {
            'article_id': hashlib.md5(article['url'].encode()).hexdigest(),
            'source': article['source'],
            'url': article['url'],
            'title': article['title'],
            'published_at': article['published_at'],
            'summary': summary,
            'top_facts': top_facts,
            'simhash': article['simhash'],
            'updated_at': datetime.now()
        }
        
        processed_articles.append(article_record)
        total_tokens += estimate_tokens(full_content) + estimate_tokens(summary)
    
    # Step 5: DB ì ì¬
    inserted_count = upsert_articles_to_db(processed_articles)
    
    # ë¡œê·¸ ê¸°ë¡
    log_batch_run(run_id, len(raw_articles), inserted_count, start_time)
    
    return {
        'run_id': run_id,
        'discovered': len(raw_articles),
        'deduplicated': len(normalized_articles),
        'inserted': inserted_count,
        'total_tokens': total_tokens,
        'elapsed_seconds': (datetime.now() - start_time).total_seconds()
    }


def on_demand_query(user_query: str, feed_urls: List[str]) -> Dict:
    """
    ì˜¨ë””ë§¨ë“œ ì¿¼ë¦¬ ì²˜ë¦¬
    1. ì‹¤ì‹œê°„ RSS ì¡°íšŒ
    2. ìµœì‹  Nê±´ ë³¸ë¬¸ ì¶”ì¶œ
    3. LLMì— ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì „ë‹¬
    4. ì‘ë‹µ ìƒì„±
    """
    # ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘
    recent_articles = fetch_rss_articles(feed_urls, limit_per_feed=5)
    
    # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_docs = []
    for article in recent_articles[:5]:
        content = extract_article_body(article['url'])
        context_docs.append({
            'title': article['title'],
            'source': article['source'],
            'url': article['url'],
            'snippet': content[:500]  # ì²« 500ì
        })
    
    # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹¤ì œë¡œëŠ” API í˜¸ì¶œ)
    prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ê´€ë ¨ ìµœì‹  ê¸°ì‚¬:
{chr(10).join([f"[{doc['source']}] {doc['title']}: {doc['snippet']}..." for doc in context_docs])}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
    
    # Mock LLM response
    response = f"'{user_query}'ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤. ìµœê·¼ ë‰´ìŠ¤ë¥¼ ë¶„ì„í•œ ê²°ê³¼, " \
               f"ê²½ì œ ì§€í‘œê°€ ì „ë°˜ì ìœ¼ë¡œ ê¸ì •ì ì¸ íë¦„ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. " \
               f"íŠ¹íˆ ìˆ˜ì¶œê³¼ ê³ ìš© ì§€í‘œê°€ ê°œì„ ë˜ê³  ìˆìŠµë‹ˆë‹¤."
    
    citations = [
        f"[{doc['source']}] {doc['title']}\n{doc['url']}" 
        for doc in context_docs[:3]
    ]
    
    return {
        'query': user_query,
        'response': response,
        'citations': citations,
        'tokens_used': estimate_tokens(prompt) + estimate_tokens(response)
    }


# ============================================================================
# STREAMLIT UI
# ============================================================================

def main():
    st.set_page_config(
        page_title="ë‰´ìŠ¤ ì—ì´ì „íŠ¸ í”„ë¡œí† íƒ€ì…",
        page_icon="ğŸ“°",
        layout="wide"
    )
    
    # Initialize session state
    if 'db_articles' not in st.session_state:
        st.session_state.db_articles = []
    if 'fetch_logs' not in st.session_state:
        st.session_state.fetch_logs = []
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # Header
    st.title("ğŸ“° ë‰´ìŠ¤ ì—ì´ì „íŠ¸ (í”„ë¡œí† íƒ€ì…) â€” Streamlit ì´ˆì•ˆ")
    st.markdown("**AI ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘Â·ìš”ì•½Â·ê²€ìƒ‰ ì‹œìŠ¤í…œ**")
    st.divider()
    
    # ========================================================================
    # SIDEBAR: ì„¤ì •
    # ========================================================================
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        st.subheader("RSS í”¼ë“œ ì†ŒìŠ¤")
        rss_feeds = st.text_area(
            "RSS URL (ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)",
            value="\n".join(DEFAULT_RSS_FEEDS),
            height=120
        )
        feed_list = [f.strip() for f in rss_feeds.split("\n") if f.strip()]
        
        st.subheader("ë°°ì¹˜ ì„¤ì •")
        batch_interval = st.slider(
            "ë°°ì¹˜ ì£¼ê¸° (ë¶„)",
            min_value=10,
            max_value=120,
            value=BATCH_INTERVAL_MINUTES,
            step=10
        )
        
        articles_per_feed = st.number_input(
            "í”¼ë“œë‹¹ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜",
            min_value=5,
            max_value=50,
            value=10
        )
        
        st.subheader("ìš”ì•½ ì˜µì…˜")
        include_keywords = st.checkbox("í‚¤ì›Œë“œ ì¶”ì¶œ í¬í•¨", value=True)
        include_metrics = st.checkbox("í•µì‹¬ ì§€í‘œ ê°•ì¡°", value=True)
        
        st.divider()
        
        st.subheader("ğŸ“Š í† í° ì‚°ì •ê¸°")
        sample_text = st.text_area(
            "ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì…ë ¥",
            placeholder="í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
            height=100
        )
        
        if sample_text:
            tokens = estimate_tokens(sample_text)
            st.metric("ì˜ˆìƒ í† í° ìˆ˜", f"{tokens:,}")
            st.caption(f"ë¬¸ì ìˆ˜: {len(sample_text):,}ì")
    
    # ========================================================================
    # MAIN AREA: íƒ­ êµ¬ì„±
    # ========================================================================
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ”„ ë°°ì¹˜ ì‹¤í–‰", 
        "ğŸ’¬ ì˜¨ë””ë§¨ë“œ ì¿¼ë¦¬", 
        "ğŸ“Š ëŒ€ì‹œë³´ë“œ", 
        "ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤"
    ])
    
    # ------------------------------------------------------------------------
    # TAB 1: ë°°ì¹˜í˜• í”Œë¡œìš°
    # ------------------------------------------------------------------------
    with tab1:
        st.header("A) ë°°ì¹˜í˜• íŒŒì´í”„ë¼ì¸")
        
        st.markdown("""
        **ë™ì‘ íë¦„:**
        1. ğŸ“¥ RSS í”¼ë“œì—ì„œ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘
        2. ğŸ” SimHash ê¸°ë°˜ ì¤‘ë³µ ì œê±°
        3. ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ (readability)
        4. ğŸ¤– LLM ìš”ì•½ ìƒì„± (3ë¬¸ì¥ + í•µì‹¬ ì§€í‘œ)
        5. ğŸ’¾ PostgreSQL DB ì ì¬ (UPSERT)
        6. ğŸ“ ì‹¤í–‰ ë¡œê·¸ ê¸°ë¡
        """)
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            if st.button("ğŸš€ ë°°ì¹˜ ì‹¤í–‰", type="primary", use_container_width=True):
                with st.spinner("ë°°ì¹˜ ì‘ì—… ì§„í–‰ ì¤‘..."):
                    result = batch_process_pipeline(feed_list, articles_per_feed)
                    st.success("âœ… ë°°ì¹˜ ì™„ë£Œ!")
                    
                    st.metric("ë°œê²¬ëœ ê¸°ì‚¬", f"{result['discovered']}ê±´")
                    st.metric("ì¤‘ë³µ ì œê±° í›„", f"{result['deduplicated']}ê±´")
                    st.metric("DB ì‚½ì…", f"{result['inserted']}ê±´")
                    st.metric("ì´ í† í° ì‚¬ìš©", f"{result['total_tokens']:,}")
                    st.caption(f"ì†Œìš” ì‹œê°„: {result['elapsed_seconds']:.2f}ì´ˆ")
        
        with col2:
            st.info(f"""
            **í˜„ì¬ ì„¤ì •:**
            - RSS ì†ŒìŠ¤: {len(feed_list)}ê°œ
            - í”¼ë“œë‹¹ ìˆ˜ì§‘: {articles_per_feed}ê±´
            - ë°°ì¹˜ ì£¼ê¸°: {batch_interval}ë¶„
            - ì˜ˆìƒ í† í°/íšŒ: ~{articles_per_feed * len(feed_list) * 1500:,}
            """)
        
        st.divider()
        
        st.subheader("ğŸ“‹ ìµœê·¼ ë°°ì¹˜ ë¡œê·¸")
        if st.session_state.fetch_logs:
            df_logs = pd.DataFrame(st.session_state.fetch_logs)
            df_logs['duration'] = (df_logs['finished_at'] - df_logs['started_at']).dt.total_seconds()
            st.dataframe(
                df_logs[['run_id', 'started_at', 'discovered_count', 'inserted_count', 'duration']],
                use_container_width=True
            )
        else:
            st.caption("ì•„ì§ ì‹¤í–‰ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # ------------------------------------------------------------------------
    # TAB 2: ì˜¨ë””ë§¨ë“œ ì¿¼ë¦¬
    # ------------------------------------------------------------------------
    with tab2:
        st.header("B) ì˜¨ë””ë§¨ë“œ ì¿¼ë¦¬ (ì±—ë´‡)")
        
        st.markdown("""
        ìµœì‹  ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """)
        
        # Chat interface
        user_query = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: ì˜¤ëŠ˜ ê¸ˆìœµì‹œì¥ ë™í–¥ì€?"
        )
        
        if st.button("ğŸ” ì§ˆì˜í•˜ê¸°", type="primary"):
            if user_query:
                with st.spinner("ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¶„ì„ ì¤‘..."):
                    result = on_demand_query(user_query, feed_list)
                    
                    st.session_state.chat_history.append({
                        'timestamp': datetime.now(),
                        'query': user_query,
                        'response': result['response'],
                        'citations': result['citations'],
                        'tokens': result['tokens_used']
                    })
        
        st.divider()
        
        # Display chat history
        if st.session_state.chat_history:
            st.subheader("ğŸ’¬ ëŒ€í™” ë‚´ì—­")
            for i, chat in enumerate(reversed(st.session_state.chat_history[-5:])):
                with st.expander(
                    f"ğŸ• {chat['timestamp'].strftime('%H:%M:%S')} - {chat['query'][:50]}...",
                    expanded=(i == 0)
                ):
                    st.markdown(f"**ì§ˆë¬¸:** {chat['query']}")
                    st.markdown(f"**ë‹µë³€:** {chat['response']}")
                    
                    st.caption("**ì¶œì²˜:**")
                    for citation in chat['citations']:
                        st.caption(citation)
                    
                    st.caption(f"í† í° ì‚¬ìš©: {chat['tokens']:,}")
        else:
            st.info("ì•„ì§ ëŒ€í™” ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”!")
    
    # ------------------------------------------------------------------------
    # TAB 3: ëŒ€ì‹œë³´ë“œ
    # ------------------------------------------------------------------------
    with tab3:
        st.header("ğŸ“Š ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "ì´ ê¸°ì‚¬ ìˆ˜",
                f"{len(st.session_state.db_articles):,}ê±´"
            )
        
        with col2:
            recent_articles = [
                a for a in st.session_state.db_articles
                if (datetime.now() - a['published_at']).total_seconds() < 10800  # 3ì‹œê°„
            ]
            st.metric(
                "ìµœê·¼ 3ì‹œê°„",
                f"{len(recent_articles)}ê±´"
            )
        
        with col3:
            total_runs = len(st.session_state.fetch_logs)
            st.metric(
                "ë°°ì¹˜ ì‹¤í–‰ íšŸìˆ˜",
                f"{total_runs}íšŒ"
            )
        
        with col4:
            total_queries = len(st.session_state.chat_history)
            st.metric(
                "ì´ ì¿¼ë¦¬ ìˆ˜",
                f"{total_queries}ê±´"
            )
        
        st.divider()
        
        # Recent articles preview
        st.subheader("ğŸ“° ìµœê·¼ ìˆ˜ì§‘ ê¸°ì‚¬ (ìµœëŒ€ 10ê±´)")
        if st.session_state.db_articles:
            recent_df = pd.DataFrame(st.session_state.db_articles[-10:])
            
            for _, article in recent_df.iterrows():
                with st.container():
                    col_a, col_b = st.columns([3, 1])
                    with col_a:
                        st.markdown(f"**[{article['source']}] {article['title']}**")
                        st.caption(article['summary'])
                    with col_b:
                        st.caption(f"ğŸ• {article['published_at'].strftime('%m/%d %H:%M')}")
                        st.caption(f"[ë§í¬]({article['url']})")
                    
                    if article.get('top_facts'):
                        with st.expander("ğŸ“Œ í•µì‹¬ íŒ©íŠ¸"):
                            st.markdown(article['top_facts'])
                    
                    st.divider()
        else:
            st.info("ì•„ì§ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë°°ì¹˜ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”!")
    
    # ------------------------------------------------------------------------
    # TAB 4: ë°ì´í„°ë² ì´ìŠ¤
    # ------------------------------------------------------------------------
    with tab4:
        st.header("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë° ìš´ì˜")
        
        st.subheader("ğŸ“‹ í…Œì´ë¸” êµ¬ì¡°")
        
        with st.expander("**articles** í…Œì´ë¸”", expanded=True):
            st.code("""
CREATE TABLE articles (
    article_id VARCHAR(64) PRIMARY KEY,
    source VARCHAR(100),
    url TEXT UNIQUE,
    title TEXT,
    published_at TIMESTAMP,
    summary TEXT,
    top_facts TEXT,
    simhash BIGINT,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_published_at ON articles(published_at DESC);
CREATE INDEX idx_source ON articles(source);
CREATE INDEX idx_simhash ON articles(simhash);
            """, language="sql")
        
        with st.expander("**fetch_logs** í…Œì´ë¸”"):
            st.code("""
CREATE TABLE fetch_logs (
    run_id VARCHAR(64) PRIMARY KEY,
    started_at TIMESTAMP,
    finished_at TIMESTAMP,
    discovered_count INT,
    inserted_count INT,
    error_message TEXT
);
            """, language="sql")
        
        with st.expander("**model_runs** í…Œì´ë¸” (LLM ì¶”ì ìš©)"):
            st.code("""
CREATE TABLE model_runs (
    run_id VARCHAR(64) PRIMARY KEY,
    article_id VARCHAR(64) REFERENCES articles(article_id),
    model_name VARCHAR(100),
    prompt_tokens INT,
    completion_tokens INT,
    total_cost DECIMAL(10, 6),
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
            """, language="sql")
        
        st.divider()
        
        st.subheader("ğŸ”§ ìš´ì˜ ê³ ë ¤ì‚¬í•­")
        st.markdown("""
        **1. ìŠ¤ì¼€ì¤„ë§**
        - Cron job ë˜ëŠ” APScheduler ì‚¬ìš©
        - ë°°ì¹˜ ì‹¤í–‰ ì‹œ ì¤‘ë³µ ë°©ì§€ (ë¶„ì‚° ë½)
        
        **2. ì—ëŸ¬ ì²˜ë¦¬**
        - RSS íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡ ë° ì¬ì‹œë„
        - LLM API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ exponential backoff
        
        **3. ì„±ëŠ¥ ìµœì í™”**
        - ë³¸ë¬¸ ì¶”ì¶œ ë³‘ë ¬ ì²˜ë¦¬ (asyncio, ThreadPoolExecutor)
        - LLM ë°°ì¹˜ ìš”ì²­ìœ¼ë¡œ ë¹„ìš© ì ˆê°
        - Redis ìºì‹œë¡œ ì¤‘ë³µ URL ì²´í¬ ê°€ì†í™”
        
        **4. ëª¨ë‹ˆí„°ë§**
        - Prometheus + Grafanaë¡œ ë©”íŠ¸ë¦­ ì¶”ì 
        - í† í° ì‚¬ìš©ëŸ‰, ì‘ë‹µ ì‹œê°„, ì—ëŸ¬ìœ¨ ëª¨ë‹ˆí„°ë§
        
        **5. ë°ì´í„° ë³´ê´€ ì •ì±…**
        - 90ì¼ ì´ìƒ ëœ ê¸°ì‚¬ ì•„ì¹´ì´ë¹™
        - ë¡œê·¸ í…Œì´ë¸” ì£¼ê¸°ì  ì •ë¦¬
        """)
        
        st.divider()
        
        st.subheader("ğŸ“Š í˜„ì¬ DB ìƒíƒœ ë¯¸ë¦¬ë³´ê¸°")
        if st.session_state.db_articles:
            df_preview = pd.DataFrame(st.session_state.db_articles)
            st.dataframe(
                df_preview[['article_id', 'source', 'title', 'published_at']].head(20),
                use_container_width=True
            )
            
            # Download button
            csv = df_preview.to_csv(index=False).encode('utf-8-sig')
            st.download_button(
                label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"news_articles_{datetime.now().strftime('%Y%m%d')}.csv",
                mime="text/csv"
            )
        else:
            st.caption("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ========================================================================
    # FOOTER
    # ========================================================================
    st.divider()
    st.caption("""
    **í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ í•„ìš”í•œ ì‘ì—…:**
    - `feedparser`, `readability-lxml` ì‹¤ì œ ì—°ë™
    - OpenAI/Anthropic API í‚¤ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    - PostgreSQL ì—°ê²° (psycopg2 ë˜ëŠ” SQLAlchemy)
    - Docker ì»¨í…Œì´ë„ˆí™” ë° CI/CD íŒŒì´í”„ë¼ì¸
    - ë³´ì•ˆ: API í‚¤ ê´€ë¦¬, Rate limiting, CORS ì„¤ì •
    """)


if __name__ == "__main__":
    main()